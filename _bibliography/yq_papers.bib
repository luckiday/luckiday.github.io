@article{yang2025socialmind,
  abbr={IMWUT'25},
  title={Socialmind: Llm-based proactive ar social assistive system with human-like perception for in-situ live interactions},
  author={Yang*, Bufang and Guo*, Yunqi and Xu*, Lilin and Yan, Zhenyu and Chen, Hongkai and Xing, Guoliang and Jiang, Xiaofan},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={9},
  number={1},
  pages={1--30},
  year={2025},
  publisher={ACM New York, NY, USA},
  dimensions={false},
  selected={true},
  note={*Co-first author}
}

@article{guo2024sensor2,
  abbr={FMSys'24},
  title={Sensor2Scene: Foundation Model-driven Interactive Realities},
  author={Guo, Yunqi and Hou, Kaiyuan and Yan, Zhenyu and Chen, Hongkai and Xing, Guoliang and Jiang, Xiaofan},
  abstract={Augmented Reality (AR) is acclaimed for its potential to bridge the physical and virtual worlds. Yet, current integration between these realms often lacks a deep understanding of the physical environment and the subsequent scene generation that reflects this understanding. This research introduces Sensor2Scene, a novel system framework designed to enhance user interactions with sensor data through AR. At its core, an AI agent leverages large language models (LLMs) to decode subtle information from sensor data, constructing detailed scene descriptions for visualization. To enable these scenes to be rendered in AR, we decompose the scene creation process into tasks of text-to-3D model generation and spatial composition, allowing new AR scenes to be sketched from the descriptions.
    We evaluated our framework using an LLM evaluator based on five metrics on various datasets to examine the correlation between sensor readings and corresponding visualizations, and demonstrated the system's effectiveness with scenes generated from end-to-end.
    The results highlight the potential of LLMs to understand IoT sensor data. Furthermore, generative models can aid in transforming these interpretations into visual formats, thereby enhancing user interaction. This work not only displays the capabilities of Sensor2Scene but also lays a foundation for advancing AR with the goal of creating more immersive and contextually rich experiences.},
  pdf={sensor2scene-fmsys2024.pdf},
  slides={sensor2scene-slides.pdf},
  journal={FMSys},
  year={2024},
  dimensions={false},
  selected={false}
}

@article{guo2023sign911,
  abbr={MobiCom'23},
  title={Sign-to-911: Emergency Call Service for Sign Language Users with Assistive AR Glasses},
  author={Guo, Yunqi and Zhao, Jinghao and Ding, Boyan and Tan, Congkai and Ling, Weichong and Tan, Zhaowei and Du, Hongzhe and Miyaki, Jennifer and Lu, Songwu},
  abstract={Sign-to-911 offers a compact mobile system solution to fast and runtime American Sign Language (ASL) and English translations. It is designated as 911 call services for ASL users with hearing disabilities upon emergencies. It enables bidirectional translations of ASL-to-English and English-to-ASL. The signer wears the AR glasses, runs Sign-to-911 on his/her smartphone and glasses, and interacts with a 911 operator. The design of Sign-to-911 departs from the popular deep learning based solution paradigm, and adopts simpler traditional AI/machine learning (ML) models. The key is to exploit ASL linguistic features to simplify the model structures and improve accuracy and speed. It further leverages recent component solutions from graphics, vision, natural language processing, and AI/ML. Our evaluation with six ASL signers and 911 call records has confirmed its viability.},
  slides={sign-to-911-mobicom23.pdf},
  journal={MobiCom},
  numpages={14},
  year={2023},
  dimensions={false},
  selected={true}
}

@article{tan2023ldrp,
  abbr={TMC'23},
  title={LDRP: Device-Centric Latency Diagnostic and Reduction for Cellular Networks without Root},
  author={Tan, Zhaowei and Zhao, Jinghao and Li, Yuanjie and Xu, Yifei and Guo, Yunqi and Lu, Songwu},
  journal={IEEE TMC},
  year={2023},
  selected={true}
}

@article{guo2021modelobfuscation,
  abbr={CNS'21},
  title={A Model Obfuscation Approach to IoT Security},
  author={Guo, Yunqi and Tan, Zhaowei and Chen, Kaiyuan and Lu, Songwu and Wu, Ying Nian},
  journal={IEEE CNS},
  year={2021},
  note={Co-first author},
  selected={true}
}

@article{raza2021keyreinstallation,
  abbr={ACSAC'21},
  title={On Key Reinstallation Attacks over 4G LTE Control-Plane: Feasibility and Negative Impact},
  author={Raza, Muhammad Taqi and Guo, Yunqi and Lu, Songwu and Anwar, Fatima Muhammad},
  journal={ACM ACSAC},
  year={2021},
  selected={true}
}

@article{zhao2021securesim,
  abbr={MobiCom'21},
  title={SecureSIM: Rethinking Authentication and Access Control for SIM/eSIM},
  author={Zhao, Jinghao and Ding, Boyan and Guo, Yunqi and Tan, Zhaowei and Lu, Songwu},
  journal={AMC MobiCom},
  year={2021},
  selected={true}
}

@article{tan2021dataplane,
  abbr={MobiCom'21},
  title={Data-Plane Signaling in Cellular IoT: Attacks and Defense},
  author={Tan, Zhaowei and Ding, Boyan and Zhao, Jinghao and Guo, Yunqi and Lu, Songwu},
  journal={ACM MobiCom},
  year={2021},
  selected={true}
}

@article{li2021experience,
  abbr={MobiCom'21},
  title={Experience: a Five-Year Retrospective of MobileInsight},
  author={Li, Yuanjie and Peng, Chunyi and Zhang, Zhehui and Tan, Zhaowei and Deng, Haotian and Zhao, Jinghao and Li, Qianru and Guo, Yunqi and Ling, Kai and Ding, Boyan and Li, Hewu and Lu, Songwu},
  journal={ACM MobiCom},
  year={2021},
  selected={true}
}

@article{guo2020modelcentric,
  abbr={ICCCN'20},
  title={Towards Model-Centric Security for IoT Systems},
  author={Guo, Yunqi and Tan, Zhaowei and Lu, Songwu},
  journal={IEEE ICCCN},
  year={2020},
  note={Invited paper},
  selected={true}
}
